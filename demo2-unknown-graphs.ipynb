{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import flow2graph\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_labels = Path(\"./Datasets/CTU-13-Extended/labels.json\")\n",
    "datasets = flow2graph.NetflowDataset.load_from_labels(path_labels, \n",
    "                                                      window_time_sec=120,\n",
    "                                                      chunksize=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(g, k=2, min_degree=1, inplace=True):\n",
    "    g = g if inplace else g.copy()\n",
    "    if k > 0:\n",
    "        bt = nx.degree_centrality(g)\n",
    "        bt = sorted(bt.items() , key=lambda x: x[-1])[::-1]\n",
    "        g.remove_nodes_from(map(lambda x: x[0], bt[:k]))\n",
    "    if min_degree > 0:\n",
    "        to_remove = list(map(lambda d: d[0], filter(lambda d: d[1] < min_degree, g.degree)))\n",
    "        g.remove_nodes_from(to_remove)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "!mkdir graphs-wo-noise\n",
    "\n",
    "root = Path('./graphs-wo-noise')\n",
    "counter = 0\n",
    "n_labels = None\n",
    "for dt in tqdm(datasets.values()):\n",
    "    for (i, df) in tqdm(enumerate(dt), leave=False): \n",
    "        if (df.label.values == flow2graph.Label.malicious.value).sum() < 10:\n",
    "            continue\n",
    "        df = dt.compute_features(df, normalize=True)\n",
    "        g = dt.to_graph(df)\n",
    "#         g = remove_noise(g, k=2)\n",
    "        nx.write_gpickle(g, path=root/f\"{counter:05d}.pkl\")\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "from dgl.data.utils import save_graphs, load_graphs, download\n",
    "\n",
    "class CTU13Dataset(DGLDataset):\n",
    "    from functools import cached_property\n",
    "    \n",
    "    url = \"https://filebin.net/cy19fulrva0t71n3/ctu13.tar.gz?t=jj1h0gl9\"\n",
    "        \n",
    "    label_normal = flow2graph.Label.normal.value\n",
    "    label_background = flow2graph.Label.background.value\n",
    "    label_malicious = flow2graph.Label.malicious.value\n",
    "    \n",
    "    def __init__(self, url=None, raw_dir=None, save_dir=None, force_reload=False, verbose=False):\n",
    "        self._save_dir = Path(save_dir) # (1) temporary fix waiting for: https://github.com/dmlc/dgl/pull/2262\n",
    "        self._raw_dir = Path(raw_dir)\n",
    "        \n",
    "        self._p_raws = sorted(list(self._raw_dir.glob('[0-9]*.pkl')))\n",
    "        self._p_unraws = list(map(lambda p: self._save_dir/f\"{p.name.rstrip('pkl')}nx\", self._p_raws))\n",
    "        \n",
    "        super().__init__(name='CTU13',\n",
    "                         url=url, \n",
    "                         raw_dir=self._raw_dir, \n",
    "                         save_dir=123, # (1)\n",
    "                         force_reload=force_reload, \n",
    "                         verbose=verbose)\n",
    "    def process(self):\n",
    "        self._save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for i in tqdm(range(len(self._p_raws)), desc='processing', disable=self.verbose):\n",
    "            rp, wp = self._p_raws[i], self._p_unraws[i]\n",
    "            \n",
    "            g = nx.read_gpickle(rp)\n",
    "            g = dgl.from_networkx(g, node_attrs=['label'], edge_attrs=['features', 'label'])\n",
    "            \n",
    "            label_edge, label_node = g.edata.pop('label'), g.ndata.pop('label')\n",
    "            save_graphs(str(wp), g, labels={\n",
    "                'edge': label_edge,\n",
    "                'node': label_node,\n",
    "            })\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError\n",
    "        g, labels = load_graphs(str(self._p_unraws[idx]))\n",
    "        return g[0], labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._p_unraws)\n",
    "        \n",
    "    def has_cache(self):\n",
    "        return len(self) == len(list(self._save_dir.glob('[0-9]*.nx')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.utils import split_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "dt = CTU13Dataset(raw_dir='./graphs-noise/', save_dir='/tmp/dgl-test')\n",
    "dt_train, dt_val, dt_test = split_dataset(dt, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1383, 172, 174)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt_train), len(dt_val), len(dt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EdgeToNode(nn.Module):\n",
    "    def __init__(self, in_features, out_features, non_linear=None):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(in_features, out_features)\n",
    "        self.linear_out = nn.Linear(in_features, out_features)\n",
    "        self.non_linear = non_linear or nn.Identity()\n",
    "        \n",
    "    def forward(self, graph: dgl.DGLGraph, h):\n",
    "        h_in, h_out = self.linear_in(h), self.linear_out(h)\n",
    "        h_in, h_out = self.non_linear(h_in), self.non_linear(h_out)\n",
    "        \n",
    "        with graph.local_scope(): \n",
    "            graph.edata['e_in'] = h_in\n",
    "            graph.edata['e_out'] = h_out\n",
    "            \n",
    "            # copying `e_in` edge feature to dst node + aggregating with sum into `n_in`\n",
    "            graph.update_all(fn.copy_e('e_in', 'n_in'), fn.sum('n_in', 'n_in')) \n",
    "            \n",
    "            # reversing the graph so that src nodes become dst nodes\n",
    "            r_graph = graph.reverse(copy_ndata=True, copy_edata=True) \n",
    "            # copying `e_out` edge feature to src node + aggregating with sum into `n_in`\n",
    "            r_graph.update_all(fn.copy_e('e_out', 'n_out'), fn.sum('n_out', 'n_out')) \n",
    "            \n",
    "            return torch.tanh(graph.ndata['n_in'] + r_graph.ndata['n_out'])\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(in_feats=in_feats, out_feats=hid_feats, aggregator_type='pool')\n",
    "        self.conv2 = dglnn.SAGEConv(in_feats=hid_feats, out_feats=out_feats, aggregator_type='pool')\n",
    "\n",
    "    def forward(self, graph, h):    \n",
    "        h = F.relu(self.conv1(graph, h))\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "    \n",
    "class NodeToEdge(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, non_linear=None):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.Linear(in_features=in_feats, out_features=hid_feats)\n",
    "        self.lin_out = nn.Linear(in_features=in_feats, out_features=hid_feats)\n",
    "        self.lin_final = nn.Linear(in_features=hid_feats, out_features=out_feats)\n",
    "        self.non_linear = non_linear or nn.Identity()\n",
    "        \n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.edata['h'] = torch.zeros((graph.number_of_edges(), h.shape[-1]))\n",
    "            \n",
    "            graph.apply_edges(dgl.function.e_add_u('h', 'h', 'h_out'))\n",
    "            graph.apply_edges(dgl.function.e_add_v('h', 'h', 'h_in'))\n",
    "            \n",
    "            h_out = self.lin_out(graph.edata['h_out'])\n",
    "            h_in = self.lin_in(graph.edata['h_in'])\n",
    "            \n",
    "            h_out = self.non_linear(h_out)\n",
    "            h_in = self.non_linear(h_in)\n",
    "            \n",
    "            return self.lin_final(h_out + h_in)\n",
    "\n",
    "class EdgePredictor(nn.Module):\n",
    "    def __init__(self, in_features_e, out_features_e=16, hid_features_n=32, out_features_n=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.e2n = EdgeToNode(in_features=in_features_e, out_features=out_features_e, non_linear=nn.ReLU())\n",
    "        self.lin1 = nn.Linear(in_features=out_features_e, out_features=out_features_e)\n",
    "        self.sage = SAGE(in_feats=out_features_e, hid_feats=hid_features_n, out_feats=out_features_n)\n",
    "        self.lin2 = nn.Linear(in_features=out_features_n, out_features=out_features_n)\n",
    "        self.n2y = nn.Linear(in_features=out_features_n, out_features=2)\n",
    "        self.n2e = NodeToEdge(in_feats=out_features_n, hid_feats=hid_features_n, out_feats=2)\n",
    "        \n",
    "    def forward(self, graph, h):    \n",
    "        h = self.e2n(graph, h)\n",
    "        h = self.lin1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.sage(graph, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        pred_node = self.n2y(h)\n",
    "        pred_edge = self.n2e(graph, h)\n",
    "        \n",
    "        return pred_node, pred_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1f983e49e341a5a1efe9581a5e9d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1383.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0082, 0.9918])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_malicious, nb_tot = 0, 0\n",
    "nb_malicious_n, nb_tot_n = 0, 0\n",
    "for (_, labels) in tqdm(dt_train):\n",
    "    y = labels['edge'] == CTU13Dataset.label_malicious\n",
    "    nb_malicious += y.sum().item()\n",
    "    nb_tot += len(y)\n",
    "    \n",
    "ratio = nb_malicious/nb_tot\n",
    "weights = 1.0-torch.tensor([1.0-ratio, ratio])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=174.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test      : loss:0.060 | acc_n:0.015 | acc_e: 0.894\n"
     ]
    }
   ],
   "source": [
    "# model = EdgePredictor(5, out_features_e=16, hid_features_n=32, out_features_n=16)\n",
    "loss_test, acc_test_n, acc_test_e = evaluate(model, dt_test)\n",
    "print(f\"Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=174.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before train:\n",
      "\t Validation: loss:0.659 | acc_n:0.000 | acc_e: 0.987\n",
      "\t Test      : loss:0.660 | acc_n:0.000 | acc_e: 0.991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396146f0faa0447ab13c6abe4c4e4ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epoch', max=10.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss_train=0.115 loss_val=0.047 | acc_train_n=0.681 acc_val_n=0.109 | acc_train_e=0.938 acc_val_e=0.934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss_train=0.067 loss_val=0.031 | acc_train_n=0.814 acc_val_n=0.110 | acc_train_e=0.976 acc_val_e=0.975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: loss_train=0.049 loss_val=0.036 | acc_train_n=0.839 acc_val_n=0.112 | acc_train_e=0.986 acc_val_e=0.978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: loss_train=0.045 loss_val=0.039 | acc_train_n=0.853 acc_val_n=0.101 | acc_train_e=0.983 acc_val_e=0.976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: loss_train=0.046 loss_val=0.037 | acc_train_n=0.865 acc_val_n=0.108 | acc_train_e=0.984 acc_val_e=0.979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: loss_train=0.038 loss_val=0.052 | acc_train_n=0.882 acc_val_n=0.110 | acc_train_e=0.988 acc_val_e=0.954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: loss_train=0.032 loss_val=0.035 | acc_train_n=0.904 acc_val_n=0.116 | acc_train_e=0.990 acc_val_e=0.963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: loss_train=0.030 loss_val=0.031 | acc_train_n=0.878 acc_val_n=0.116 | acc_train_e=0.990 acc_val_e=0.963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: loss_train=0.029 loss_val=0.033 | acc_train_n=0.878 acc_val_n=0.116 | acc_train_e=0.995 acc_val_e=0.970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: loss_train=0.028 loss_val=0.042 | acc_train_n=0.897 acc_val_n=0.110 | acc_train_e=0.993 acc_val_e=0.933\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=174.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After train:\n",
      "\t Validation: loss:0.042 | acc_n:0.110 | acc_e: 0.933\n",
      "\t Test      : loss:0.049 | acc_n:0.106 | acc_e: 0.909\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = EdgePredictor(5, out_features_e=16, hid_features_n=32, out_features_n=16)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "cst_wl_edge = 0.7\n",
    "assert cst_wl_edge >= 0.0 and cst_wl_edge <= 1.0, \"edges weighting loss must belong to [0, 1]\"\n",
    "\n",
    "def compute_loss(y_pred_n, y_pred_e, y_true_n, y_true_e):\n",
    "    loss_n = criterion(y_pred_n, y_true_n)\n",
    "    loss_e = criterion(y_pred_e, y_true_e)\n",
    "    return (1.0 - cst_wl_edge) * loss_n + cst_wl_edge * loss_e\n",
    "\n",
    "def evaluate(model, dt):\n",
    "    losses = []\n",
    "    acc_e, acc_n = 0, 0\n",
    "    nb_e, nb_n = 0, 0\n",
    "    # validation loop\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (g, labels) in tqdm(dt, desc='eval', leave=False):\n",
    "#             mask_node = labels['node'] != CTU13Dataset.label_background\n",
    "#             mask_edge = labels['edge'] != CTU13Dataset.label_background\n",
    "            label_node = (labels['node'] == CTU13Dataset.label_malicious).long()\n",
    "            label_edge = (labels['edge'] == CTU13Dataset.label_malicious).long()\n",
    "\n",
    "            ft_edges = g.edata['features']\n",
    "\n",
    "            pred_node, pred_edge = tuple(map(lambda y: y.squeeze(), model(g, ft_edges)))\n",
    "            loss = compute_loss(pred_node, pred_edge, label_node, label_edge)\n",
    "            \n",
    "            # that's not really the accuracy, but True Positives (malicious)\n",
    "            acc_e += (pred_edge.max(dim=-1)[1])[label_edge.bool()].sum().item()\n",
    "            acc_n += (pred_node.max(dim=-1)[1])[label_node.bool()].sum().item()\n",
    "            nb_e += label_edge.sum().item()\n",
    "            nb_n += label_node.sum().item()\n",
    "            \n",
    "            losses.append(loss.item()) \n",
    "    return np.mean(losses), acc_n/nb_e, acc_e/nb_e\n",
    "    \n",
    "def train(model, dt_train, dt_val, dt_test, nb_epochs=10, freq_show_loss=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    loss_val, acc_val_n, acc_val_e = evaluate(model, dt_val)\n",
    "    loss_test, acc_test_n, acc_test_e = evaluate(model, dt_test)\n",
    "    print(\"Before train:\")\n",
    "    print(f\"\\t Validation: loss:{loss_val:.3f} | acc_n:{acc_val_n:.3f} | acc_e: {acc_val_e:.3f}\")\n",
    "    print(f\"\\t Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\")       \n",
    "    \n",
    "    for epoch in tqdm(range(nb_epochs), desc='epoch'):\n",
    "        losses = []\n",
    "        \n",
    "        acc_train_e, acc_train_n = 0, 0\n",
    "        nb_train_e, nb_train_n = 0, 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # training loop\n",
    "        pb_training = tqdm(dt_train, desc='train', leave=False)\n",
    "        for idx, (g, labels) in enumerate(pb_training):\n",
    "#             mask_n = labels['node'] != CTU13Dataset.label_background\n",
    "#             mask_e = labels['edge'] != CTU13Dataset.label_background\n",
    "            label_n = (labels['node'] == CTU13Dataset.label_malicious).long()\n",
    "            label_e = (labels['edge'] == CTU13Dataset.label_malicious).long()\n",
    "            \n",
    "            ft_e = g.edata['features']\n",
    "\n",
    "            # criterion\n",
    "            pred_n, pred_e = model(g, ft_e)\n",
    "            pred_n, pred_e = pred_n.squeeze(), pred_e.squeeze()\n",
    "            loss = compute_loss(y_pred_n=pred_n, y_pred_e=pred_e,\n",
    "                               y_true_n=label_n, y_true_e=label_e)\n",
    "            \n",
    "            # loss monitoring\n",
    "            acc_train_e += (pred_e.max(dim=-1)[-1])[label_e.bool()].sum().item()\n",
    "            acc_train_n += (pred_n.max(dim=-1)[-1])[label_n.bool()].sum().item()\n",
    "            nb_train_e += label_e.sum().item()\n",
    "            nb_train_n += label_n.sum().item()\n",
    "            losses.append(loss.item())\n",
    "            if idx % freq_show_loss == 0:\n",
    "                pb_training.set_description(f\"loss: {np.mean(losses):.2f}, \\\n",
    "                acc_e:{acc_train_e/nb_train_e:.2f}, \\\n",
    "                acc_n:{acc_train_n/nb_train_n:.2f}\")\n",
    "            \n",
    "            # weights optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        loss_train, acc_train_n, acc_train_e = np.mean(losses), acc_train_n/nb_train_n, acc_train_e/nb_train_e\n",
    "        loss_val, acc_val_n, acc_val_e = evaluate(model, dt_val)\n",
    "        print(f\"epoch {epoch}: loss_train={loss_train:.3f} loss_val={loss_val:.3f} | \"\n",
    "              f\"acc_train_n={acc_train_n:.3f} acc_val_n={acc_val_n:.3f} | \"\n",
    "              f\"acc_train_e={acc_train_e:.3f} acc_val_e={acc_val_e:.3f}\") \n",
    "\n",
    "    loss_val, acc_val_n, acc_val_e = evaluate(model, dt_val)\n",
    "    loss_test, acc_test_n, acc_test_e = evaluate(model, dt_test)\n",
    "    print(\"After train:\")\n",
    "    print(f\"\\t Validation: loss:{loss_val:.3f} | acc_n:{acc_val_n:.3f} | acc_e: {acc_val_e:.3f}\")\n",
    "    print(f\"\\t Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\")       \n",
    "    \n",
    "train(model, dt_train, dt_val, dt_test, nb_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Nodes Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       1.00      1.00      1.00      2529\n",
      "   malicious       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           1.00      2530\n",
      "   macro avg       0.62      1.00      0.70      2530\n",
      "weighted avg       1.00      1.00      1.00      2530\n",
      "\n",
      "Eval Edges Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       1.00      1.00      1.00      4936\n",
      "   malicious       0.55      1.00      0.71        24\n",
      "\n",
      "    accuracy                           1.00      4960\n",
      "   macro avg       0.77      1.00      0.85      4960\n",
      "weighted avg       1.00      1.00      1.00      4960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "g, labels = dt_test[120]\n",
    "ft_edges = g.edata['features']\n",
    "mask = labels['edge'] != CTU13Dataset.label_background\n",
    "y_true_n = labels['node'] == CTU13Dataset.label_malicious\n",
    "y_true_e = labels['edge'] == CTU13Dataset.label_malicious\n",
    "with torch.no_grad():\n",
    "    y_pred_n, y_pred_e = model(g, ft_edges)\n",
    "    _, y_pred_n = y_pred_n.max(dim=-1)\n",
    "    _, y_pred_e = y_pred_e.max(dim=-1)\n",
    "print('Eval Nodes Classification:')\n",
    "print(classification_report(y_true_n, y_pred_n, target_names=['normal', 'malicious']))\n",
    "print('Eval Edges Classification:')\n",
    "print(classification_report(y_true_e, y_pred_e, target_names=['normal', 'malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 120),\n",
       " (21, 112),\n",
       " (19, 127),\n",
       " (19, 24),\n",
       " (18, 131),\n",
       " (18, 116),\n",
       " (18, 86),\n",
       " (18, 71),\n",
       " (18, 39),\n",
       " (18, 11),\n",
       " (17, 146),\n",
       " (17, 31),\n",
       " (17, 14),\n",
       " (16, 121),\n",
       " (16, 64),\n",
       " (16, 46),\n",
       " (16, 44),\n",
       " (16, 29),\n",
       " (16, 1),\n",
       " (15, 172),\n",
       " (15, 171),\n",
       " (15, 117),\n",
       " (15, 94),\n",
       " (15, 49),\n",
       " (15, 34),\n",
       " (15, 8),\n",
       " (15, 4),\n",
       " (14, 154),\n",
       " (14, 149),\n",
       " (14, 111),\n",
       " (14, 66),\n",
       " (14, 36),\n",
       " (14, 7),\n",
       " (13, 141),\n",
       " (12, 142),\n",
       " (12, 54),\n",
       " (12, 26),\n",
       " (11, 114),\n",
       " (11, 99),\n",
       " (11, 69),\n",
       " (11, 56),\n",
       " (11, 41),\n",
       " (10, 167),\n",
       " (10, 164),\n",
       " (10, 161),\n",
       " (10, 124),\n",
       " (10, 76),\n",
       " (9, 136),\n",
       " (9, 132),\n",
       " (9, 107),\n",
       " (9, 91),\n",
       " (9, 89),\n",
       " (9, 74),\n",
       " (9, 61),\n",
       " (9, 16),\n",
       " (9, 9),\n",
       " (8, 166),\n",
       " (8, 159),\n",
       " (8, 151),\n",
       " (8, 139),\n",
       " (8, 134),\n",
       " (8, 129),\n",
       " (8, 126),\n",
       " (8, 119),\n",
       " (8, 109),\n",
       " (8, 102),\n",
       " (8, 101),\n",
       " (8, 84),\n",
       " (8, 79),\n",
       " (8, 59),\n",
       " (8, 51),\n",
       " (8, 19),\n",
       " (7, 144),\n",
       " (7, 125),\n",
       " (7, 122),\n",
       " (7, 118),\n",
       " (7, 115),\n",
       " (7, 103),\n",
       " (7, 97),\n",
       " (7, 82),\n",
       " (7, 81),\n",
       " (7, 67),\n",
       " (7, 52),\n",
       " (7, 37),\n",
       " (7, 35),\n",
       " (7, 22),\n",
       " (7, 21),\n",
       " (7, 20),\n",
       " (7, 6),\n",
       " (7, 5),\n",
       " (6, 162),\n",
       " (6, 157),\n",
       " (6, 140),\n",
       " (6, 110),\n",
       " (6, 106),\n",
       " (6, 104),\n",
       " (6, 95),\n",
       " (6, 93),\n",
       " (6, 88),\n",
       " (6, 80),\n",
       " (6, 78),\n",
       " (6, 77),\n",
       " (6, 73),\n",
       " (6, 65),\n",
       " (6, 63),\n",
       " (6, 58),\n",
       " (6, 50),\n",
       " (6, 48),\n",
       " (6, 43),\n",
       " (6, 33),\n",
       " (6, 28),\n",
       " (6, 18),\n",
       " (6, 13),\n",
       " (6, 3),\n",
       " (5, 170),\n",
       " (5, 168),\n",
       " (5, 165),\n",
       " (5, 163),\n",
       " (5, 155),\n",
       " (5, 152),\n",
       " (5, 150),\n",
       " (5, 148),\n",
       " (5, 133),\n",
       " (5, 123),\n",
       " (5, 108),\n",
       " (5, 105),\n",
       " (5, 96),\n",
       " (5, 92),\n",
       " (5, 90),\n",
       " (5, 75),\n",
       " (5, 72),\n",
       " (5, 60),\n",
       " (5, 45),\n",
       " (5, 30),\n",
       " (5, 15),\n",
       " (5, 0),\n",
       " (4, 169),\n",
       " (4, 153),\n",
       " (4, 138),\n",
       " (4, 137),\n",
       " (4, 135),\n",
       " (4, 128),\n",
       " (4, 100),\n",
       " (4, 62),\n",
       " (4, 57),\n",
       " (4, 47),\n",
       " (4, 42),\n",
       " (4, 32),\n",
       " (4, 27),\n",
       " (4, 17),\n",
       " (4, 12),\n",
       " (4, 10),\n",
       " (4, 2),\n",
       " (3, 156),\n",
       " (3, 113),\n",
       " (3, 87),\n",
       " (3, 85),\n",
       " (3, 70),\n",
       " (3, 55),\n",
       " (3, 40),\n",
       " (3, 25),\n",
       " (2, 173),\n",
       " (2, 158),\n",
       " (2, 147),\n",
       " (2, 143),\n",
       " (2, 130),\n",
       " (2, 98),\n",
       " (2, 83),\n",
       " (2, 68),\n",
       " (2, 53),\n",
       " (2, 38),\n",
       " (2, 23),\n",
       " (1, 160),\n",
       " (1, 145)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = []\n",
    "for i, (_, l) in enumerate(dt_test):\n",
    "    ml = (l['edge']==CTU13Dataset.label_malicious).sum().item()\n",
    "    b.append((ml, i))\n",
    "sorted(b)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(in_feats=n_features, hid_feats=100, out_feats=n_labels)\n",
    "opt = th.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "model.train()\n",
    "t = tqdm(range(10))\n",
    "for epoch in t:\n",
    "    logits = model(graph, node_features)\n",
    "    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
    "    acc = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    t.set_description(f\"val_acc: {acc:0.3f}, loss: {loss.item():0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, graph, node_features, node_labels, valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.linear(in_features * 2, out_classes)\n",
    "    def apply_edges(self, edges):    \n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(th.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.sage = SAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = DotProductPredictor()\n",
    "    def forward(self, g, x):\n",
    "        h = self.sage(g, x)\n",
    "        h = self.pred(g, h)\n",
    "        h = th.sigmoid(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.random.randint(0, 100, 500)\n",
    "dst = np.random.randint(0, 100, 500)\n",
    "# make it symmetric\n",
    "edge_pred_graph = dgl.graph((np.concatenate([src, dst]), np.concatenate([dst, src])))\n",
    "# synthetic node and edge features, as well as edge labels\n",
    "edge_pred_graph.ndata['feature'] = th.randn(100, 10)\n",
    "edge_pred_graph.edata['feature'] = th.randn(1000, 10)\n",
    "edge_pred_graph.edata['label'] = th.randn(1000)\n",
    "# synthetic train-validation-test splits\n",
    "edge_pred_graph.edata['train_mask'] = th.zeros(1000, dtype=th.bool).bernoulli(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = edge_pred_graph.ndata['feature']\n",
    "edge_label = edge_pred_graph.edata['label']\n",
    "train_mask = edge_pred_graph.edata['train_mask']\n",
    "model = Model(10, 32, 16)\n",
    "opt = th.optim.Adam(model.parameters())\n",
    "t = tqdm(range(100))\n",
    "for epoch in t:\n",
    "    pred = model(edge_pred_graph, node_features)\n",
    "    loss = ((pred[train_mask] - edge_label[train_mask]) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    t.set_description(f\"loss: {loss.item():.03f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
