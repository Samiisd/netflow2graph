{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import flow2graph\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_labels = Path(\"./Datasets/CTU-13-Extended/labels.json\")\n",
    "datasets = flow2graph.NetflowDataset.load_from_labels(path_labels, \n",
    "                                                      window_time_sec=120,\n",
    "                                                      chunksize=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(g, k=2, min_degree=1, inplace=True):\n",
    "    g = g if inplace else g.copy()\n",
    "    if k > 0:\n",
    "        bt = nx.degree_centrality(g)\n",
    "        bt = sorted(bt.items() , key=lambda x: x[-1])[::-1]\n",
    "        g.remove_nodes_from(map(lambda x: x[0], bt[:k]))\n",
    "    if min_degree > 0:\n",
    "        to_remove = list(map(lambda d: d[0], filter(lambda d: d[1] < min_degree, g.degree)))\n",
    "        g.remove_nodes_from(to_remove)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "!mkdir graphs-wo-noise\n",
    "\n",
    "root = Path('./graphs-wo-noise')\n",
    "counter = 0\n",
    "n_labels = None\n",
    "for dt in tqdm(datasets.values()):\n",
    "    for (i, df) in tqdm(enumerate(dt), leave=False): \n",
    "        if (df.label.values == flow2graph.Label.malicious.value).sum() < 10:\n",
    "            continue\n",
    "        df = dt.compute_features(df, normalize=True)\n",
    "        g = dt.to_graph(df)\n",
    "        g = remove_noise(g, k=2)\n",
    "        nx.write_gpickle(g, path=root/f\"{counter:05d}.pkl\")\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "from dgl.data.utils import save_graphs, load_graphs, download\n",
    "\n",
    "class CTU13Dataset(DGLDataset):\n",
    "    from functools import cached_property\n",
    "    \n",
    "    url = \"https://filebin.net/cy19fulrva0t71n3/ctu13.tar.gz?t=jj1h0gl9\"\n",
    "        \n",
    "    label_normal = flow2graph.Label.normal.value\n",
    "    label_background = flow2graph.Label.background.value\n",
    "    label_malicious = flow2graph.Label.malicious.value\n",
    "    \n",
    "    def __init__(self, url=None, raw_dir=None, save_dir=None, force_reload=False, verbose=False):\n",
    "        self._save_dir = Path(save_dir) # (1) temporary fix waiting for: https://github.com/dmlc/dgl/pull/2262\n",
    "        self._raw_dir = Path(raw_dir)\n",
    "        \n",
    "        self._p_raws = sorted(list(self._raw_dir.glob('[0-9]*.pkl')))\n",
    "        self._p_unraws = list(map(lambda p: self._save_dir/f\"{p.name.rstrip('pkl')}nx\", self._p_raws))\n",
    "        \n",
    "        super().__init__(name='CTU13',\n",
    "                         url=url, \n",
    "                         raw_dir=self._raw_dir, \n",
    "                         save_dir=123, # (1)\n",
    "                         force_reload=force_reload, \n",
    "                         verbose=verbose)\n",
    "    def process(self):\n",
    "        self._save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for i in tqdm(range(len(self._p_raws)), desc='processing', disable=self.verbose):\n",
    "            rp, wp = self._p_raws[i], self._p_unraws[i]\n",
    "            \n",
    "            g = nx.read_gpickle(rp)\n",
    "            g = dgl.from_networkx(g, node_attrs=['label'], edge_attrs=['features', 'label'])\n",
    "            \n",
    "            label_edge, label_node = g.edata.pop('label'), g.ndata.pop('label')\n",
    "            save_graphs(str(wp), g, labels={\n",
    "                'edge': label_edge,\n",
    "                'node': label_node,\n",
    "            })\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError\n",
    "        g, labels = load_graphs(str(self._p_unraws[idx]))\n",
    "        return g[0], labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._p_unraws)\n",
    "        \n",
    "    def has_cache(self):\n",
    "        return len(self) == len(list(self._save_dir.glob('[0-9]*.nx')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.utils import split_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "dt = CTU13Dataset(raw_dir='./graphs-noise/', save_dir='/tmp/dgl-test5')\n",
    "dt_train, dt_val, dt_test = split_dataset(dt, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1383, 172, 174)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt_train), len(dt_val), len(dt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EdgeToNode(nn.Module):\n",
    "    def __init__(self, in_features, out_features, non_linear=None):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(in_features, out_features)\n",
    "        self.linear_out = nn.Linear(in_features, out_features)\n",
    "        self.non_linear = non_linear or nn.Identity()\n",
    "        \n",
    "    def forward(self, graph: dgl.DGLGraph, h):\n",
    "        h_in, h_out = self.linear_in(h), self.linear_out(h)\n",
    "        h_in, h_out = self.non_linear(h_in), self.non_linear(h_out)\n",
    "        \n",
    "        with graph.local_scope(): \n",
    "            graph.edata['e_in'] = h_in\n",
    "            graph.edata['e_out'] = h_out\n",
    "            \n",
    "            # copying `e_in` edge feature to dst node + aggregating with sum into `n_in`\n",
    "            graph.update_all(fn.copy_e('e_in', 'n_in'), fn.sum('n_in', 'n_in')) \n",
    "            \n",
    "            # reversing the graph so that src nodes become dst nodes\n",
    "            r_graph = graph.reverse(copy_ndata=True, copy_edata=True) \n",
    "            # copying `e_out` edge feature to src node + aggregating with sum into `n_in`\n",
    "            r_graph.update_all(fn.copy_e('e_out', 'n_out'), fn.sum('n_out', 'n_out')) \n",
    "            \n",
    "            return torch.tanh(graph.ndata['n_in'] + r_graph.ndata['n_out'])\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(in_feats=in_feats, out_feats=hid_feats, aggregator_type='pool')\n",
    "        self.conv2 = dglnn.SAGEConv(in_feats=hid_feats, out_feats=out_feats, aggregator_type='pool')\n",
    "\n",
    "    def forward(self, graph, h):    \n",
    "        h = F.relu(self.conv1(graph, h))\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "    \n",
    "class NodeToEdge(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, non_linear=None):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.Linear(in_features=in_feats, out_features=hid_feats)\n",
    "        self.lin_out = nn.Linear(in_features=in_feats, out_features=hid_feats)\n",
    "        self.lin_final = nn.Linear(in_features=hid_feats, out_features=out_feats)\n",
    "        self.non_linear = non_linear or nn.Identity()\n",
    "        \n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.edata['h'] = torch.zeros((graph.number_of_edges(), h.shape[-1]))\n",
    "            \n",
    "            graph.apply_edges(dgl.function.e_add_u('h', 'h', 'h_out'))\n",
    "            graph.apply_edges(dgl.function.e_add_v('h', 'h', 'h_in'))\n",
    "            \n",
    "            h_out = self.lin_out(graph.edata['h_out'])\n",
    "            h_in = self.lin_in(graph.edata['h_in'])\n",
    "            \n",
    "            h_out = self.non_linear(h_out)\n",
    "            h_in = self.non_linear(h_in)\n",
    "            \n",
    "            return self.lin_final(h_out + h_in)\n",
    "\n",
    "class EdgePredictor(nn.Module):\n",
    "    def __init__(self, in_features_e, out_features_e=16, hid_features_n=32, out_features_n=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.e2n = EdgeToNode(in_features=in_features_e, out_features=out_features_e, non_linear=nn.ReLU())\n",
    "        self.lin1 = nn.Linear(in_features=out_features_e, out_features=out_features_e)\n",
    "        self.sage = SAGE(in_feats=out_features_e, hid_feats=hid_features_n, out_feats=out_features_n)\n",
    "        self.lin2 = nn.Linear(in_features=out_features_n, out_features=out_features_n)\n",
    "        self.n2y = nn.Linear(in_features=out_features_n, out_features=2)\n",
    "        self.n2e = NodeToEdge(in_feats=out_features_n, hid_feats=hid_features_n, out_feats=2)\n",
    "        \n",
    "    def forward(self, graph, h):    \n",
    "        h = self.e2n(graph, h)\n",
    "        h = self.lin1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.sage(graph, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        pred_node = self.n2y(h)\n",
    "        pred_edge = self.n2e(graph, h)\n",
    "        \n",
    "        return pred_node, pred_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483a2bc549884d9fb4bc975a1c718f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1383.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0075, 0.9925])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_malicious, nb_tot = 0, 0\n",
    "nb_malicious_n, nb_tot_n = 0, 0\n",
    "for (_, labels) in tqdm(dt_train):\n",
    "    y = labels['edge'] == CTU13Dataset.label_malicious\n",
    "    nb_malicious += y.sum().item()\n",
    "    nb_tot += len(y)\n",
    "    \n",
    "ratio = nb_malicious/nb_tot\n",
    "weights = 1.0-torch.tensor([1.0-ratio, ratio])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=174.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test      : loss:0.060 | acc_n:0.015 | acc_e: 0.894\n"
     ]
    }
   ],
   "source": [
    "# model = EdgePredictor(5, out_features_e=16, hid_features_n=32, out_features_n=16)\n",
    "loss_test, acc_test_n, acc_test_e = evaluate(model, dt_test)\n",
    "print(f\"Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=174.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before train:\n",
      "\t Validation: loss:0.655 | acc_n:0.000 | acc_e: 0.418\n",
      "\t Test      : loss:0.656 | acc_n:0.000 | acc_e: 0.318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719477a8adbc4890ab4d611f1bf4cc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epoch', max=10.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss_train=0.114 loss_val=0.040 | acc_train_n=0.591 acc_val_n=0.018 | acc_train_e=0.877 acc_val_e=0.976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss_train=0.040 loss_val=0.037 | acc_train_n=0.816 acc_val_n=0.019 | acc_train_e=0.984 acc_val_e=0.967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: loss_train=0.029 loss_val=0.023 | acc_train_n=0.850 acc_val_n=0.019 | acc_train_e=0.993 acc_val_e=0.949\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: loss_train=0.035 loss_val=0.028 | acc_train_n=0.864 acc_val_n=0.018 | acc_train_e=0.982 acc_val_e=0.976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: loss_train=0.021 loss_val=0.020 | acc_train_n=0.888 acc_val_n=0.018 | acc_train_e=0.994 acc_val_e=0.982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: loss_train=0.024 loss_val=0.019 | acc_train_n=0.898 acc_val_n=0.020 | acc_train_e=0.993 acc_val_e=0.990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: loss_train=0.023 loss_val=0.019 | acc_train_n=0.895 acc_val_n=0.019 | acc_train_e=0.988 acc_val_e=0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: loss_train=0.017 loss_val=0.021 | acc_train_n=0.928 acc_val_n=0.019 | acc_train_e=0.995 acc_val_e=0.984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: loss_train=0.019 loss_val=0.021 | acc_train_n=0.924 acc_val_n=0.020 | acc_train_e=0.996 acc_val_e=0.985\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1383.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='eval', max=172.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: loss_train=0.016 loss_val=0.027 | acc_train_n=0.918 acc_val_n=0.020 | acc_train_e=0.995 acc_val_e=0.979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-a92d7394fa16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-154-a92d7394fa16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dt_train, dt_val, dt_test, nb_epochs, freq_show_loss)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminiters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminiters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Try to detect if there was an error or KeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# in manual mode: if n < total, things probably got wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;31m# decrement instance pos and remove from internal set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decr_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;31m# GUI mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m_decr_instances\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instances\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m                     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/_monitor.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_killed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlgraph/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlgraph/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = EdgePredictor(5, out_features_e=16, hid_features_n=32, out_features_n=16)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "cst_wl_edge = 0.7\n",
    "assert cst_wl_edge >= 0.0 and cst_wl_edge <= 1.0, \"edges weighting loss must belong to [0, 1]\"\n",
    "\n",
    "def compute_loss(y_pred_n, y_pred_e, y_true_n, y_true_e):\n",
    "    loss_n = criterion(y_pred_n, y_true_n)\n",
    "    loss_e = criterion(y_pred_e, y_true_e)\n",
    "    return (1.0 - cst_wl_edge) * loss_n + cst_wl_edge * loss_e\n",
    "\n",
    "def evaluate(model, dt):\n",
    "    losses = []\n",
    "    acc_e, acc_n = 0, 0\n",
    "    nb_e, nb_n = 0, 0\n",
    "    # validation loop\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (g, labels) in tqdm(dt, desc='eval', leave=False):\n",
    "#             mask_node = labels['node'] != CTU13Dataset.label_background\n",
    "#             mask_edge = labels['edge'] != CTU13Dataset.label_background\n",
    "            label_node = (labels['node'] == CTU13Dataset.label_malicious).long()\n",
    "            label_edge = (labels['edge'] == CTU13Dataset.label_malicious).long()\n",
    "\n",
    "            ft_edges = g.edata['features']\n",
    "\n",
    "            pred_node, pred_edge = tuple(map(lambda y: y.squeeze(), model(g, ft_edges)))\n",
    "            loss = compute_loss(pred_node, pred_edge, label_node, label_edge)\n",
    "            \n",
    "            # that's not really the accuracy, but True Positives (malicious)\n",
    "            acc_e += (pred_edge.max(dim=-1)[1])[label_edge.bool()].sum().item()\n",
    "            acc_n += (pred_node.max(dim=-1)[1])[label_node.bool()].sum().item()\n",
    "            nb_e += label_edge.sum().item()\n",
    "            nb_n += label_node.sum().item()\n",
    "            \n",
    "            losses.append(loss.item()) \n",
    "    return np.mean(losses), acc_n/nb_e, acc_e/nb_e\n",
    "    \n",
    "def train(model, dt_train, dt_val, dt_test, nb_epochs=10, freq_show_loss=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    loss_val, acc_val_n, acc_val_e = evaluate(model, dt_val)\n",
    "    loss_test, acc_test_n, acc_test_e = evaluate(model, dt_test)\n",
    "    print(\"Before train:\")\n",
    "    print(f\"\\t Validation: loss:{loss_val:.3f} | acc_n:{acc_val_n:.3f} | acc_e: {acc_val_e:.3f}\")\n",
    "    print(f\"\\t Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\")       \n",
    "    \n",
    "    for epoch in tqdm(range(nb_epochs), desc='epoch'):\n",
    "        losses = []\n",
    "        \n",
    "        acc_train_e, acc_train_n = 0, 0\n",
    "        nb_train_e, nb_train_n = 0, 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # training loop\n",
    "        pb_training = tqdm(dt_train, desc='train', leave=False)\n",
    "        for idx, (g, labels) in enumerate(pb_training):\n",
    "#             mask_n = labels['node'] != CTU13Dataset.label_background\n",
    "#             mask_e = labels['edge'] != CTU13Dataset.label_background\n",
    "            label_n = (labels['node'] == CTU13Dataset.label_malicious).long()\n",
    "            label_e = (labels['edge'] == CTU13Dataset.label_malicious).long()\n",
    "            \n",
    "            ft_e = g.edata['features']\n",
    "\n",
    "            # criterion\n",
    "            pred_n, pred_e = model(g, ft_e)\n",
    "            pred_n, pred_e = pred_n.squeeze(), pred_e.squeeze()\n",
    "            loss = compute_loss(y_pred_n=pred_n, y_pred_e=pred_e,\n",
    "                               y_true_n=label_n, y_true_e=label_e)\n",
    "            \n",
    "            # loss monitoring\n",
    "            acc_train_e += (pred_e.max(dim=-1)[-1])[label_e.bool()].sum().item()\n",
    "            acc_train_n += (pred_n.max(dim=-1)[-1])[label_n.bool()].sum().item()\n",
    "            nb_train_e += label_e.sum().item()\n",
    "            nb_train_n += label_n.sum().item()\n",
    "            losses.append(loss.item())\n",
    "            if idx % freq_show_loss == 0:\n",
    "                pb_training.set_description(f\"loss: {np.mean(losses):.2f}, \\\n",
    "                acc_e:{acc_train_e/nb_train_e:.2f}, \\\n",
    "                acc_n:{acc_train_n/nb_train_n:.2f}\")\n",
    "            \n",
    "            # weights optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        loss_train, acc_train_n, acc_train_e = np.mean(losses), acc_train_n/nb_train_n, acc_train_e/nb_train_e\n",
    "        loss_val, acc_val_n, acc_val_e = evaluate(model, dt_val)\n",
    "        print(f\"epoch {epoch}: loss_train={loss_train:.3f} loss_val={loss_val:.3f} | \"\n",
    "              f\"acc_train_n={acc_train_n:.3f} acc_val_n={acc_val_n:.3f} | \"\n",
    "              f\"acc_train_e={acc_train_e:.3f} acc_val_e={acc_val_e:.3f}\") \n",
    "\n",
    "    loss_val, acc_val_n, acc_val_e = evaluate(model, dt_val)\n",
    "    loss_test, acc_test_n, acc_test_e = evaluate(model, dt_test)\n",
    "    print(\"After train:\")\n",
    "    print(f\"\\t Validation: loss:{loss_val:.3f} | acc_n:{acc_val_n:.3f} | acc_e: {acc_val_e:.3f}\")\n",
    "    print(f\"\\t Test      : loss:{loss_test:.3f} | acc_n:{acc_test_n:.3f} | acc_e: {acc_test_e:.3f}\")       \n",
    "    \n",
    "train(model, dt_train, dt_val, dt_test, nb_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Nodes Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       1.00      1.00      1.00     15698\n",
      "   malicious       0.08      1.00      0.15         1\n",
      "\n",
      "    accuracy                           1.00     15699\n",
      "   macro avg       0.54      1.00      0.58     15699\n",
      "weighted avg       1.00      1.00      1.00     15699\n",
      "\n",
      "Eval Edges Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       1.00      1.00      1.00     31225\n",
      "   malicious       0.91      1.00      0.95       108\n",
      "\n",
      "    accuracy                           1.00     31333\n",
      "   macro avg       0.95      1.00      0.98     31333\n",
      "weighted avg       1.00      1.00      1.00     31333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "g, labels = dt_test[5]\n",
    "ft_edges = g.edata['features']\n",
    "mask = labels['edge'] != CTU13Dataset.label_background\n",
    "y_true_n = labels['node'] == CTU13Dataset.label_malicious\n",
    "y_true_e = labels['edge'] == CTU13Dataset.label_malicious\n",
    "with torch.no_grad():\n",
    "    y_pred_n, y_pred_e = model(g, ft_edges)\n",
    "    _, y_pred_n = y_pred_n.max(dim=-1)\n",
    "    _, y_pred_e = y_pred_e.max(dim=-1)\n",
    "print('Eval Nodes Classification:')\n",
    "print(classification_report(y_true_n, y_pred_n, target_names=['normal', 'malicious']))\n",
    "print('Eval Edges Classification:')\n",
    "print(classification_report(y_true_e, y_pred_e, target_names=['normal', 'malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "for i, (_, l) in enumerate(dt_test):\n",
    "    ml = (l['edge']==CTU13Dataset.label_malicious).sum().item()\n",
    "    b.append((ml, i))\n",
    "sorted(b)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(in_feats=n_features, hid_feats=100, out_feats=n_labels)\n",
    "opt = th.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "model.train()\n",
    "t = tqdm(range(10))\n",
    "for epoch in t:\n",
    "    logits = model(graph, node_features)\n",
    "    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
    "    acc = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    t.set_description(f\"val_acc: {acc:0.3f}, loss: {loss.item():0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, graph, node_features, node_labels, valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.linear(in_features * 2, out_classes)\n",
    "    def apply_edges(self, edges):    \n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(th.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.sage = SAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = DotProductPredictor()\n",
    "    def forward(self, g, x):\n",
    "        h = self.sage(g, x)\n",
    "        h = self.pred(g, h)\n",
    "        h = th.sigmoid(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.random.randint(0, 100, 500)\n",
    "dst = np.random.randint(0, 100, 500)\n",
    "# make it symmetric\n",
    "edge_pred_graph = dgl.graph((np.concatenate([src, dst]), np.concatenate([dst, src])))\n",
    "# synthetic node and edge features, as well as edge labels\n",
    "edge_pred_graph.ndata['feature'] = th.randn(100, 10)\n",
    "edge_pred_graph.edata['feature'] = th.randn(1000, 10)\n",
    "edge_pred_graph.edata['label'] = th.randn(1000)\n",
    "# synthetic train-validation-test splits\n",
    "edge_pred_graph.edata['train_mask'] = th.zeros(1000, dtype=th.bool).bernoulli(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = edge_pred_graph.ndata['feature']\n",
    "edge_label = edge_pred_graph.edata['label']\n",
    "train_mask = edge_pred_graph.edata['train_mask']\n",
    "model = Model(10, 32, 16)\n",
    "opt = th.optim.Adam(model.parameters())\n",
    "t = tqdm(range(100))\n",
    "for epoch in t:\n",
    "    pred = model(edge_pred_graph, node_features)\n",
    "    loss = ((pred[train_mask] - edge_label[train_mask]) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    t.set_description(f\"loss: {loss.item():.03f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlgraph)",
   "language": "python",
   "name": "mlgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
